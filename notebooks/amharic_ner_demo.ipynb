{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amharic E-commerce NER Demo\n",
    "\n",
    "This notebook demonstrates the Amharic Named Entity Recognition system for e-commerce data extraction.\n",
    "\n",
    "## Overview\n",
    "- Data collection from Telegram channels\n",
    "- Text preprocessing and entity extraction\n",
    "- Model training and evaluation\n",
    "- Vendor scorecard analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas not available. Some features may be limited.\n",
      "âœ… All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import json\n",
    "\n",
    "# Try to import optional dependencies\n",
    "try:\n",
    "    import pandas as pd\n",
    "    PANDAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Pandas not available. Some features may be limited.\")\n",
    "    PANDAS_AVAILABLE = False\n",
    "\n",
    "# Import our custom modules\n",
    "try:\n",
    "    from data_processor import AmharicDataProcessor\n",
    "    from conll_labeler import CoNLLLabeler\n",
    "    from vendor_scorecard import VendorAnalytics\n",
    "    print(\"âœ… All modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Import error: {e}\")\n",
    "    print(\"Make sure you're running from the project root directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1200 messages\n",
      "Channels: {'@meneshayeofficial', ' @ZemenExpress', '@Leyueqa', '@sinayelj', '@nevacomputer', '@ethio_brand_collection'}\n"
     ]
    }
   ],
   "source": [
    "# Load raw Telegram data\n",
    "data = []\n",
    "with open('../raw_telegram_data.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            obj = json.loads(line.strip())\n",
    "            data.append(obj)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "print(f\"Loaded {len(data)} messages\")\n",
    "print(f\"Channels: {set(item['channel'] for item in data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Message 1 ---\n",
      "Channel:  @ZemenExpress\n",
      "Text: ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\n",
      "\n",
      "ğŸ“ŒOver Door Hooks Japanese Style Door Back Hangers \n",
      "\n",
      "ğŸ‘Punch Organizers for Towels Coats and More \n",
      "ğŸ‘Easy Installation Versatile Home Storage\n",
      "\n",
      "á‹‹áŒ‹á¦Â  ğŸ’µğŸ·Â 1ááˆ¬ 300Â  á‰¥áˆ­...\n",
      "Media: media/ @ZemenExpress_7022.jpg\n",
      "\n",
      "--- Message 2 ---\n",
      "Channel:  @ZemenExpress\n",
      "Text: ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\n",
      "\n",
      "ğŸ“ŒOver Door Hooks Japanese Style Door Back Hangers \n",
      "\n",
      "ğŸ‘Punch Organizers for Towels Coats and More \n",
      "ğŸ‘Easy Installation Versatile Home Storage\n",
      "\n",
      "á‹‹áŒ‹á¦Â  ğŸ’µğŸ·Â 1ááˆ¬ 300Â  á‰¥áˆ­...\n",
      "Media: None\n",
      "\n",
      "--- Message 3 ---\n",
      "Channel:  @ZemenExpress\n",
      "Text: ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\n",
      "\n",
      "ğŸ“ŒBall Ice Cube Tray\n",
      "\n",
      "á‹‹áŒ‹á¦Â  ğŸ’° ğŸ·Â  700 á‰¥áˆ­\n",
      "\n",
      "â™¦ï¸á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹ ğŸ”¥ğŸ”¥ğŸ”¥\n",
      "\n",
      "ğŸ¢ áŠ á‹µáˆ«áˆ»ğŸ‘‰\n",
      "\n",
      "ğŸ“â™¦ï¸#áˆ˜áŒˆáŠ“áŠ›_áˆ˜áˆ°áˆ¨á‰µ_á‹°á‹áˆ­_áˆáˆ_áˆáˆˆá‰°áŠ›_áá‰… á‰¢áˆ® á‰. S05/S06\n",
      "\n",
      "\n",
      "Â Â Â Â  ğŸ’§ğŸ’§ğŸ’§ğŸ’§\n",
      "\n",
      "\n",
      "Â Â Â  ğŸ“² 0902660722\n",
      "Â Â Â  ğŸ“² 0928460606...\n",
      "Media: None\n"
     ]
    }
   ],
   "source": [
    "# Display sample messages\n",
    "for i, item in enumerate(data[:3]):\n",
    "    print(f\"\\n--- Message {i+1} ---\")\n",
    "    print(f\"Channel: {item['channel']}\")\n",
    "    print(f\"Text: {item['text'][:200]}...\")\n",
    "    print(f\"Media: {item['media']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\n",
      "Cleaned: á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆ\n",
      "\n",
      "Extracted entities: {'products': [], 'prices': ['500'], 'locations': ['á‰¦áˆŒ']}\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = AmharicDataProcessor()\n",
    "\n",
    "# Process sample text\n",
    "sample_text = \"ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\"\n",
    "clean_text = processor.normalize_amharic_text(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {clean_text}\")\n",
    "\n",
    "# Extract entities\n",
    "entities = processor.extract_entities_from_text(clean_text)\n",
    "print(f\"\\nExtracted entities: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\n",
      "Cleaned: á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆ\n",
      "\n",
      "Extracted entities: {'products': [], 'prices': ['500'], 'locations': ['á‰¦áˆŒ']}\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = AmharicDataProcessor()\n",
    "\n",
    "# Process sample text\n",
    "sample_text = \"ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\"\n",
    "clean_text = processor.normalize_amharic_text(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {clean_text}\")\n",
    "\n",
    "# Extract entities\n",
    "entities = processor.extract_entities_from_text(clean_text)\n",
    "print(f\"\\nExtracted entities: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\n",
      "Cleaned: á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆ\n",
      "\n",
      "Extracted entities: {'products': [], 'prices': ['500'], 'locations': ['á‰¦áˆŒ']}\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = AmharicDataProcessor()\n",
    "\n",
    "# Process sample text\n",
    "sample_text = \"ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\"\n",
    "clean_text = processor.normalize_amharic_text(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {clean_text}\")\n",
    "\n",
    "# Extract entities\n",
    "entities = processor.extract_entities_from_text(clean_text)\n",
    "print(f\"\\nExtracted entities: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\n",
      "Cleaned: á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆ\n",
      "\n",
      "Extracted entities: {'products': [], 'prices': ['500'], 'locations': ['á‰¦áˆŒ']}\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = AmharicDataProcessor()\n",
    "\n",
    "# Process sample text\n",
    "sample_text = \"ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\"\n",
    "clean_text = processor.normalize_amharic_text(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {clean_text}\")\n",
    "\n",
    "# Extract entities\n",
    "entities = processor.extract_entities_from_text(clean_text)\n",
    "print(f\"\\nExtracted entities: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\n",
      "Cleaned: á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆ\n",
      "\n",
      "Extracted entities: {'products': [], 'prices': ['500'], 'locations': ['á‰¦áˆŒ']}\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = AmharicDataProcessor()\n",
    "\n",
    "# Process sample text\n",
    "sample_text = \"ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\"\n",
    "clean_text = processor.normalize_amharic_text(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {clean_text}\")\n",
    "\n",
    "# Extract entities\n",
    "entities = processor.extract_entities_from_text(clean_text)\n",
    "print(f\"\\nExtracted entities: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\n",
      "Cleaned: á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆ\n",
      "\n",
      "Extracted entities: {'products': [], 'prices': ['500'], 'locations': ['á‰¦áˆŒ']}\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = AmharicDataProcessor()\n",
    "\n",
    "# Process sample text\n",
    "sample_text = \"ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“\"\n",
    "clean_text = processor.normalize_amharic_text(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {clean_text}\")\n",
    "\n",
    "# Extract entities\n",
    "entities = processor.extract_entities_from_text(clean_text)\n",
    "print(f\"\\nExtracted entities: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CoNLL Format Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\t\tLabel\n",
      "------------------------------\n",
      "á‹‹áŒ‹\t\tB-PRICE\n",
      "500\t\tI-PRICE\n",
      "á‰¥áˆ­\t\tI-PRICE\n",
      "á‹¨áˆ†áŠ\t\tO\n",
      "áˆ»áˆšá‹\t\tB-PRODUCT\n",
      "á‰ á‰¦áˆŒ\t\tB-LOC\n",
      "á‹­áˆ¸áŒ£áˆ\t\tO\n"
     ]
    }
   ],
   "source": [
    "# Initialize CoNLL labeler\n",
    "labeler = CoNLLLabeler()\n",
    "\n",
    "# Label sample text\n",
    "sample_amharic = \"á‹‹áŒ‹ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆ\"\n",
    "labeled_tokens = labeler.auto_label_text(sample_amharic)\n",
    "\n",
    "print(\"Token\\t\\tLabel\")\n",
    "print(\"-\" * 30)\n",
    "for token, label in labeled_tokens:\n",
    "    print(f\"{token}\\t\\t{label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Statistics ===\n",
      "Total sentences: 3,216\n",
      "Total tokens: 174,695\n",
      "\n",
      "Entity counts:\n",
      "  PRODUCT: 14,399\n",
      "  PRICE: 8,204\n",
      "  LOC: 2,920\n"
     ]
    }
   ],
   "source": [
    "# Load labeled dataset statistics\n",
    "try:\n",
    "    stats = labeler.validate_conll_format(\"../Data/merged_labeled_data.txt\")\n",
    "    \n",
    "    print(\"=== Dataset Statistics ===\")\n",
    "    print(f\"Total sentences: {stats['total_sentences']:,}\")\n",
    "    print(f\"Total tokens: {stats['total_tokens']:,}\")\n",
    "    print(\"\\nEntity counts:\")\n",
    "    for entity, count in stats['entity_counts'].items():\n",
    "        print(f\"  {entity}: {count:,}\")\n",
    "    \n",
    "    if stats['errors']:\n",
    "        print(f\"\\nErrors found: {len(stats['errors'])}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Labeled dataset not found. Run the labeling script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vendor Scorecard Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1200 messages\n",
      "\n",
      "=== Vendor Metrics ===\n",
      "\n",
      "ZemenExpress:\n",
      "  Posts per week: 38.89\n",
      "  Average price: 842.86 ETB\n",
      "  Price consistency: 42.00%\n",
      "  Media ratio: 81.00%\n",
      "\n",
      "nevacomputer:\n",
      "  Posts per week: 3.45\n",
      "  Average price: 0 ETB\n",
      "  Price consistency: 0.00%\n",
      "  Media ratio: 98.00%\n",
      "\n",
      "meneshayeofficial:\n",
      "  Posts per week: 4.28\n",
      "  Average price: 6670.59 ETB\n",
      "  Price consistency: 9.00%\n",
      "  Media ratio: 48.00%\n",
      "\n",
      "ethio_brand_collection:\n",
      "  Posts per week: 8.92\n",
      "  Average price: 0 ETB\n",
      "  Price consistency: 0.00%\n",
      "  Media ratio: 99.00%\n",
      "\n",
      "Leyueqa:\n",
      "  Posts per week: 36.84\n",
      "  Average price: 2226.04 ETB\n",
      "  Price consistency: 34.00%\n",
      "  Media ratio: 71.00%\n",
      "\n",
      "sinayelj:\n",
      "  Posts per week: 73.68\n",
      "  Average price: 6350.0 ETB\n",
      "  Price consistency: 4.00%\n",
      "  Media ratio: 98.00%\n"
     ]
    }
   ],
   "source": [
    "# Initialize vendor analytics\n",
    "analytics = VendorAnalytics()\n",
    "\n",
    "# Load and process data\n",
    "telegram_data = analytics.load_telegram_data(\"../raw_telegram_data.jsonl\")\n",
    "print(f\"Processed {len(telegram_data)} messages\")\n",
    "\n",
    "# Calculate vendor metrics\n",
    "vendor_metrics = analytics.calculate_vendor_metrics(telegram_data)\n",
    "\n",
    "print(\"\\n=== Vendor Metrics ===\")\n",
    "for vendor, metrics in vendor_metrics.items():\n",
    "    print(f\"\\n{vendor}:\")\n",
    "    print(f\"  Posts per week: {metrics['posts_per_week']}\")\n",
    "    print(f\"  Average price: {metrics['avg_price_etb']} ETB\")\n",
    "    print(f\"  Price consistency: {metrics['price_consistency']:.2%}\")\n",
    "    print(f\"  Media ratio: {metrics['media_ratio']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vendor Scorecard ===\n",
      "Vendor                    Lending Score   Posts/Week   Avg Price   \n",
      "----------------------------------------------------------------------\n",
      "sinayelj                  79.0            73.7         6350.0      \n",
      "ZemenExpress              74.5            38.9         842.9       \n",
      "Leyueqa                   73.3            36.8         2226.0      \n",
      "ethio_brand_collection    70.3            8.9          0.0         \n",
      "nevacomputer              60.9            3.5          0.0         \n",
      "meneshayeofficial         58.6            4.3          6670.6      \n"
     ]
    }
   ],
   "source": [
    "# Create vendor scorecard\n",
    "scorecard_data = analytics.create_vendor_scorecard(telegram_data)\n",
    "\n",
    "print(\"=== Vendor Scorecard ===\")\n",
    "print(f\"{'Vendor':<25} {'Lending Score':<15} {'Posts/Week':<12} {'Avg Price':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for vendor_data in scorecard_data:\n",
    "    print(f\"{vendor_data['Vendor']:<25} {vendor_data['Lending_Score']:<15.1f} \"\n",
    "          f\"{vendor_data['Posts_Per_Week']:<12.1f} {vendor_data['Avg_Price_ETB']:<12.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training Demo (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Training Configuration ===\n",
      "Models to train:\n",
      "  - XLM-Roberta-base (multilingual)\n",
      "  - mBERT (multilingual BERT)\n",
      "  - DistilBERT (lightweight)\n",
      "\n",
      "Training parameters:\n",
      "  - Learning rate: 2e-5\n",
      "  - Batch size: 16\n",
      "  - Max length: 128\n",
      "  - Epochs: 3\n",
      "\n",
      "Dataset split:\n",
      "  - Training: 80%\n",
      "  - Validation: 20%\n"
     ]
    }
   ],
   "source": [
    "# Note: This is a conceptual demonstration\n",
    "# Actual model training requires GPU resources and transformers library\n",
    "\n",
    "print(\"=== Model Training Configuration ===\")\n",
    "print(\"Models to train:\")\n",
    "print(\"  - XLM-Roberta-base (multilingual)\")\n",
    "print(\"  - mBERT (multilingual BERT)\")\n",
    "print(\"  - DistilBERT (lightweight)\")\n",
    "print(\"\\nTraining parameters:\")\n",
    "print(\"  - Learning rate: 2e-5\")\n",
    "print(\"  - Batch size: 16\")\n",
    "print(\"  - Max length: 128\")\n",
    "print(\"  - Epochs: 3\")\n",
    "print(\"\\nDataset split:\")\n",
    "print(\"  - Training: 80%\")\n",
    "print(\"  - Validation: 20%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Project Results Summary ===\n",
      "\n",
      " Data Collection:\n",
      "   - 1200 messages from 6 channels\n",
      "   - 992 media files\n",
      "\n",
      " Data Processing:\n",
      "   - Amharic text normalization\n",
      "   - Entity pattern extraction\n",
      "   - CoNLL format labeling\n",
      "\n",
      " Vendor Analytics:\n",
      "   - 6 vendors analyzed\n",
      "   - Top vendor: sinayelj (Score: 79.0)\n",
      "\n",
      " Model Framework:\n",
      "   - Multi-model training pipeline\n",
      "   - Evaluation and comparison framework\n",
      "   - Interpretability tools (SHAP/LIME)\n",
      "\n",
      " Business Value:\n",
      "   - Automated entity extraction\n",
      "   - Vendor performance scoring\n",
      "   - Micro-lending decision support\n",
      "   - Scalable e-commerce intelligence\n"
     ]
    }
   ],
   "source": [
    "# Summary of achievements\n",
    "print(\"=== Project Results Summary ===\")\n",
    "print(\"\\n Data Collection:\")\n",
    "print(f\"   - {len(data)} messages from 6 channels\")\n",
    "print(f\"   - {len([d for d in data if d.get('media')])} media files\")\n",
    "\n",
    "print(\"\\n Data Processing:\")\n",
    "print(\"   - Amharic text normalization\")\n",
    "print(\"   - Entity pattern extraction\")\n",
    "print(\"   - CoNLL format labeling\")\n",
    "\n",
    "print(\"\\n Vendor Analytics:\")\n",
    "print(f\"   - {len(scorecard_data)} vendors analyzed\")\n",
    "print(f\"   - Top vendor: {scorecard_data[0]['Vendor']} (Score: {scorecard_data[0]['Lending_Score']:.1f})\")\n",
    "\n",
    "print(\"\\n Model Framework:\")\n",
    "print(\"   - Multi-model training pipeline\")\n",
    "print(\"   - Evaluation and comparison framework\")\n",
    "print(\"   - Interpretability tools (SHAP/LIME)\")\n",
    "\n",
    "print(\"\\n Business Value:\")\n",
    "print(\"   - Automated entity extraction\")\n",
    "print(\"   - Vendor performance scoring\")\n",
    "print(\"   - Micro-lending decision support\")\n",
    "print(\"   - Scalable e-commerce intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the complete Amharic e-commerce data extraction pipeline:\n",
    "\n",
    "1. **Data Collection**: Automated scraping from Telegram channels\n",
    "2. **Preprocessing**: Amharic-specific text normalization\n",
    "3. **Entity Extraction**: Pattern-based and ML-based approaches\n",
    "4. **Model Training**: Framework for transformer fine-tuning\n",
    "5. **Vendor Analytics**: Business intelligence and scoring\n",
    "\n",
    "The system provides a solid foundation for EthioMart's e-commerce platform and demonstrates the potential for AI-driven business intelligence in Ethiopian markets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
