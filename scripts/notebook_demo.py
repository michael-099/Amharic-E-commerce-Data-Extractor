"""
Notebook Demo Script - Standalone version of the Jupyter notebook functionality.
This script demonstrates the key features without requiring Jupyter or pandas.
"""

import sys
import os
import json

# Add scripts directory to path
sys.path.append(os.path.dirname(__file__))

def demo_data_loading():
    """Demonstrate data loading functionality."""
    print("=" * 60)
    print("ğŸ“Š DATA LOADING DEMO")
    print("=" * 60)
    
    try:
        # Load raw Telegram data
        data = []
        with open('raw_telegram_data.jsonl', 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    obj = json.loads(line.strip())
                    data.append(obj)
                except json.JSONDecodeError:
                    continue

        print(f"âœ… Loaded {len(data)} messages")
        
        # Get unique channels
        channels = set(item['channel'] for item in data)
        print(f"âœ… Found {len(channels)} channels:")
        for channel in sorted(channels):
            channel_data = [item for item in data if item['channel'] == channel]
            media_count = sum(1 for item in channel_data if item.get('media'))
            print(f"   - {channel}: {len(channel_data)} messages, {media_count} media files")
        
        # Display sample messages
        print(f"\nğŸ“ Sample Messages:")
        for i, item in enumerate(data[:3]):
            print(f"\n--- Message {i+1} ---")
            print(f"Channel: {item['channel']}")
            text = item['text'][:100] + "..." if len(item['text']) > 100 else item['text']
            print(f"Text: {text}")
            print(f"Media: {'Yes' if item['media'] else 'No'}")
            
        return data
        
    except FileNotFoundError:
        print("âŒ Raw data file not found. Please run the scraper first.")
        return []

def demo_text_processing():
    """Demonstrate text processing functionality."""
    print("\n" + "=" * 60)
    print("ğŸ”§ TEXT PROCESSING DEMO")
    print("=" * 60)

    try:
        from data_processor import AmharicDataProcessor

        processor = AmharicDataProcessor()

        # Sample Amharic text with emojis and mixed content
        sample_texts = [
            "ğŸ’¥ğŸ’¥á‹‹áŒ‹á¦ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆğŸ“",
            "ğŸ”¥ğŸ”¥ áŠ á‹²áˆµ áŠ á‰ á‰£ áˆ˜áŒˆáŠ“áŠ› áˆáˆ á‹áˆµáŒ¥ áˆ‹á•á‰¶á• á‹‹áŒ‹ 25000 á‰¥áˆ­ ğŸ”¥ğŸ”¥",
            "á‰¡áŠ“ 150 á‰¥áˆ­ áˆ»á‹­ 80 á‰¥áˆ­ á‰ áˆ˜áˆ­áŠ«á‰¶ @username #hashtag"
        ]

        print("Original â†’ Cleaned Text:")
        print("-" * 40)

        for i, text in enumerate(sample_texts):
            clean_text = processor.normalize_amharic_text(text)
            print(f"{i+1}. {text}")
            print(f"   â†’ {clean_text}")

            # Extract entities
            entities = processor.extract_entities_from_text(clean_text)
            if any(entities.values()):
                print(f"   ğŸ“‹ Entities: {entities}")
            print()

        # Show processing statistics
        print("ğŸ“Š Processing Statistics:")
        print(f"   - Emoji removal: âœ…")
        print(f"   - URL/username filtering: âœ…")
        print(f"   - Amharic text normalization: âœ…")
        print(f"   - Entity pattern matching: âœ…")

        print("\nâœ… Text processing completed successfully")

    except ImportError as e:
        print(f"âŒ Data processor module not available: {e}")
    except Exception as e:
        print(f"âŒ Error in text processing: {e}")

def demo_conll_labeling():
    """Demonstrate CoNLL labeling functionality."""
    print("\n" + "=" * 60)
    print("ğŸ·ï¸ CONLL LABELING DEMO")
    print("=" * 60)
    
    try:
        from conll_labeler import CoNLLLabeler
        
        labeler = CoNLLLabeler()
        
        # Sample Amharic sentences
        sample_sentences = [
            "á‹‹áŒ‹ 500 á‰¥áˆ­ á‹¨áˆ†áŠ áˆ»áˆšá‹ á‰ á‰¦áˆŒ á‹­áˆ¸áŒ£áˆ",
            "áŠ á‹²áˆµ áŠ á‰ á‰£ áˆ˜áŒˆáŠ“áŠ› áˆáˆ á‹áˆµáŒ¥ áˆ‹á•á‰¶á•",
            "á‰¡áŠ“ 150 á‰¥áˆ­ áˆ»á‹­ 80 á‰¥áˆ­ á‰ áˆ˜áˆ­áŠ«á‰¶"
        ]
        
        print("Token-Level Labeling Results:")
        print("-" * 40)
        
        for i, sentence in enumerate(sample_sentences):
            print(f"\n{i+1}. Sentence: {sentence}")
            labeled_tokens = labeler.auto_label_text(sentence)
            
            print("   Token\t\tLabel")
            print("   " + "-" * 25)
            for token, label in labeled_tokens:
                print(f"   {token:<15}\t{label}")
        
        # Check dataset statistics if available
        try:
            stats = labeler.validate_conll_format("Data/merged_labeled_data.txt")
            print(f"\nğŸ“Š Dataset Statistics:")
            print(f"   Total sentences: {stats['total_sentences']:,}")
            print(f"   Total tokens: {stats['total_tokens']:,}")
            print(f"   Entity counts: {stats['entity_counts']}")
        except FileNotFoundError:
            print("\nğŸ“Š Labeled dataset not found")
            
        print("\nâœ… CoNLL labeling completed successfully")
        
    except ImportError:
        print("âŒ CoNLL labeler module not available")

def demo_vendor_scorecard():
    """Demonstrate vendor scorecard functionality."""
    print("\n" + "=" * 60)
    print("ğŸ’¼ VENDOR SCORECARD DEMO")
    print("=" * 60)
    
    try:
        # Check if scorecard exists
        if os.path.exists('vendor_scorecard.csv'):
            print("ğŸ“Š Vendor Scorecard Results:")
            print("-" * 40)
            
            with open('vendor_scorecard.csv', 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            # Print header
            if lines:
                header = lines[0].strip().split(',')
                print(f"{'Vendor':<25} {'Score':<8} {'Posts/Week':<12} {'Avg Price':<12}")
                print("-" * 60)
                
                # Print vendor data
                for line in lines[1:]:
                    if line.strip():
                        parts = line.strip().split(',')
                        if len(parts) >= 4:
                            vendor = parts[0]
                            score = parts[6] if len(parts) > 6 else parts[1]
                            posts = parts[2] if len(parts) > 2 else "N/A"
                            price = parts[3] if len(parts) > 3 else "N/A"
                            print(f"{vendor:<25} {score:<8} {posts:<12} {price:<12}")
            
            print("\nâœ… Vendor scorecard loaded successfully")
        else:
            print("âŒ Vendor scorecard not found. Run vendor analysis first.")
            
    except Exception as e:
        print(f"âŒ Error loading vendor scorecard: {e}")

def demo_project_summary():
    """Display project summary."""
    print("\n" + "=" * 60)
    print("ğŸ¯ PROJECT SUMMARY")
    print("=" * 60)
    
    # Check what files exist
    files_status = {
        "Raw Data": os.path.exists('raw_telegram_data.jsonl'),
        "Cleaned Data": os.path.exists('cleaned_data.jsonl'),
        "Labeled Data": os.path.exists('Data/merged_labeled_data.txt'),
        "Vendor Scorecard": os.path.exists('vendor_scorecard.csv'),
        "Project Summary": os.path.exists('project_summary.txt')
    }
    
    print("ğŸ“ File Status:")
    for file_type, exists in files_status.items():
        status = "âœ…" if exists else "âŒ"
        print(f"   {status} {file_type}")
    
    # Count script files
    script_count = 0
    if os.path.exists('scripts'):
        script_count = len([f for f in os.listdir('scripts') if f.endswith('.py')])
    
    print(f"\nğŸ› ï¸ Implementation:")
    print(f"   ğŸ“œ Python Scripts: {script_count}")
    print(f"   ğŸ“Š Data Processing: {'âœ…' if files_status['Raw Data'] else 'âŒ'}")
    print(f"   ğŸ·ï¸ Entity Labeling: {'âœ…' if files_status['Labeled Data'] else 'âŒ'}")
    print(f"   ğŸ’¼ Vendor Analysis: {'âœ…' if files_status['Vendor Scorecard'] else 'âŒ'}")
    
    print(f"\nğŸ¯ Project Status:")
    completed_tasks = sum(files_status.values())
    total_tasks = len(files_status)
    completion_rate = (completed_tasks / total_tasks) * 100
    print(f"   Completion Rate: {completion_rate:.0f}% ({completed_tasks}/{total_tasks})")
    
    if completion_rate >= 80:
        print("   ğŸ‰ Project is ready for deployment!")
    elif completion_rate >= 60:
        print("   ğŸš§ Project is in good progress")
    else:
        print("   âš ï¸ More work needed")

def main():
    """Run the complete demo."""
    print("ğŸš€ AMHARIC E-COMMERCE DATA EXTRACTOR DEMO")
    print("=" * 60)
    print("This demo showcases the key functionality of the project")
    print("without requiring Jupyter notebook or all dependencies.")
    print()
    
    # Run all demos
    data = demo_data_loading()
    demo_text_processing()
    demo_conll_labeling()
    demo_vendor_scorecard()
    demo_project_summary()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ DEMO COMPLETED SUCCESSFULLY!")
    print("=" * 60)
    print("For full functionality, install all dependencies:")
    print("pip install -r requirements.txt")
    print("\nTo run individual components:")
    print("python scripts/scraper.py          # Data collection")
    print("python scripts/conll_labeler.py    # Data labeling")
    print("python scripts/vendor_scorecard.py # Vendor analysis")
    print("python scripts/project_summary.py  # Full summary")

if __name__ == "__main__":
    main()
